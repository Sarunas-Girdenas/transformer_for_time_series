{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import configparser\n",
    "from ast import literal_eval\n",
    "from hiko.dask import daskify\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch import nn\n",
    "\n",
    "# Read in config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../ml_models_for_airflow/dbs3_config.ini')\n",
    "\n",
    "db_engine = create_engine(config['AIRFLOW']['postgres_conn'])\n",
    "\n",
    "pairs_mapping = literal_eval(config['MODEL']['pairs_mapping'])\n",
    "pairs = tuple(pairs_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "GRAD_CLIPPING_VAL = 1\n",
    "BATCH_SIZE = 64\n",
    "INIT_LR = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_data_loader import Dataset\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_set = Dataset(config_location='../ml_models_for_airflow/dbs3_config.ini',\n",
    "                      pairs=pairs,\n",
    "                      seq_lenght=16,\n",
    "                      num_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = int(len(full_data_set)*0.85)\n",
    "test_set_size = len(full_data_set) - train_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = data.random_split(full_data_set,\n",
    "                                     [train_set_size, test_set_size]\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_generator = data.DataLoader(valset, batch_size=len(valset), shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(emb=16,\n",
    "                          heads=12,\n",
    "                          depth=2,\n",
    "                          num_features=3,\n",
    "                          interpolation_factor=10,\n",
    "                          dropout=0.2\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-5)\n",
    "learning_rate_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: min(i / (INIT_LR / BATCH_SIZE), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (time_series_features_encoding): Conv1d(3, 3, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  (pos_embedding): Embedding(16, 16)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=16, out_features=192, bias=False)\n",
       "        (toqueries): Linear(in_features=16, out_features=192, bias=False)\n",
       "        (tovalues): Linear(in_features=16, out_features=192, bias=False)\n",
       "        (unifyheads): Linear(in_features=192, out_features=16, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=16, out_features=192, bias=False)\n",
       "        (toqueries): Linear(in_features=16, out_features=192, bias=False)\n",
       "        (tovalues): Linear(in_features=16, out_features=192, bias=False)\n",
       "        (unifyheads): Linear(in_features=192, out_features=16, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward_layers): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dense_interpolation): DenseInterpolation()\n",
       "  (feed_forward): Linear(in_features=30, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.apply(Transformer.init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "track_epoch_loss = []\n",
    "train_auc = []\n",
    "test_auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(NUM_EPOCHS):\n",
    "    transformer.train()\n",
    "    epoch_loss = 0\n",
    "    temp_train_auc = 0\n",
    "    \n",
    "    for train_x, train_y in train_generator:\n",
    "        \n",
    "        predictions = transformer(train_x)\n",
    "        loss = criterion(predictions, train_y.view(-1, 1))\n",
    "        epoch_loss += loss.item()\n",
    "        temp_train_auc += roc_auc_score(\n",
    "            train_y.numpy(), predictions.detach().numpy())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip up\n",
    "        #torch.nn.utils.clip_grad_norm_(transformer.parameters(), GRAD_CLIPPING_VAL)\n",
    "\n",
    "        optimizer.step()\n",
    "        #learning_rate_scheduler.step()\n",
    "    \n",
    "    train_auc.append(temp_train_auc/len(train_generator))\n",
    "    print('train auc:', train_auc[-1], ' epoch:', ep)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        transformer.eval()\n",
    "        temp_test_auc = 0\n",
    "        for test_x, test_y in test_generator:\n",
    "            predictions = transformer(test_x)\n",
    "            temp_test_auc += roc_auc_score(\n",
    "                test_y.numpy(), predictions.numpy())\n",
    "\n",
    "    test_auc.append(temp_test_auc/len(test_generator))\n",
    "    print('test auc:', test_auc[-1], ' epoch:', ep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp37",
   "language": "python",
   "name": "pp37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
